{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 12 | Image processing with OpenCV and Pytesseract\n",
    "<hr>\n",
    "## Learning Objectives\n",
    "At the end of this lesson, you will be able to:\n",
    "\n",
    "- Read an image using `imread`\n",
    "\n",
    "- Convert an image to grayscale using `cv2.COLOR_BGR2GRAY`\n",
    "\n",
    "- Apply a threshold to the grayscale image using `cv2.THRESH_BINARY_INV` to produce binary images\n",
    "\n",
    "- Apply morphological transofrmations to the binary image from the thresholding, using a structuring element/kernel such as `cv2.MORPH_RECT` and morphologyEx operations such as `cv2.MORPH_DILATE` and `cv2.MORPH_OPEN`\n",
    "\n",
    "- Find and use the contours from the morph generated from the morphological transofrmation, using a retrieval mode such as `cv2.RETR_EXTERNAL` and a contour approximation method such as `cv2.CHAIN_APPROX_SIMPLE` or `cv2.CHAIN_APPROX_NONE`\n",
    "\n",
    "- Visualise the contour generated using `cv2.rectangle` to draw the contours\n",
    "\n",
    "- Use Pytesseract to read the text embedded in the contours found from the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the image\n",
    "The image is read using `cv2.imread`, before applying Gaussian Blur.\n",
    "![gray](gray.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "from PIL import Image, ImageOps\n",
    "# https://stackoverflow.com/questions/61199573/how-to-detect-the-text-above-lines-using-opencv-in-python\n",
    "\n",
    "image_path = \"Questions.jpg\"\n",
    "\n",
    "# Read the image\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "# Blurring\n",
    "imgBlur = cv2.GaussianBlur(img, (7, 7), 1)\n",
    "\n",
    "# Convert image to grayscale, which is needed for thresholding\n",
    "gray = cv2.cvtColor(imgBlur, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow(\"gray\", cv2.resize(gray,(700,850)))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold the grayscale image\n",
    "- If the pixel value is greater than a threshold value,\n",
    "it is assigned one value(which may be white), else it is assigned another value.(which may be black)\n",
    "\n",
    "    1. First argument is source image, which should be a grayscale image\n",
    "    2. Second argument is the threshold value which is used to classify the pixel values\n",
    "    3. Third argument is the maxVal which represents the value to be given if the pixel value is more than the \n",
    "threshold value\n",
    "\n",
    "- In this case, cv2.THRESH_BINARY_INV is used, which sets a pixel to 0(black) if it is greater\n",
    "than the threshold value(second argument), else set it to maxVal(third argument).\n",
    "THRESH_OTSU allows you to avoid choosing the threshold value and it it determined automatically\n",
    "\n",
    "- For the given image, notice that since the white background is greater than 0, it is set to black. Also, as the black words is 0, it is set to white. The purpose of this thresholding is so that the text is white on black background\n",
    "https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_thresholding/py_thresholding.html\n",
    "\n",
    "![thresh](thresh.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "cv2.imshow(\"thresh\", cv2.resize(thresh,(700,850)))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morphological transformations (Part 1)\n",
    "- Morphological transformations are some simple operations based on the image shape. It is \n",
    "normally performed on binary images. It needs two inputs, one is our original image, second one \n",
    "is called structuring element or kernel which decides the nature of operation. \n",
    "- `cv2.MORPH_RECT` is used to obtain a rectangular kernel.\n",
    "- For `cv2.MORPH_DILATE` (dilating), a pixel element is '1'(white) if atleast one pixel under the kernel is '1'. \n",
    "So it increases the white region in the image or size of foreground object increases\n",
    "- This step applies morphology dilate with horizontal kernel to blur text in a line together. This is \n",
    "because the white text is \"dilated\" and merges with the other text in the same line\n",
    "based on the rectangular kernel(which has 200 width and 3 height).\n",
    "![morph1](morph1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (200, 3)) #250,3\n",
    "morph = cv2.morphologyEx(thresh, cv2.MORPH_DILATE, kernel)\n",
    "cv2.imshow(\"morph1\", cv2.resize(morph,(700,850)))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morphological transformations (Part 2)\n",
    "- For erosion, pixel in the original image (either 1 or 0) will be considered 1 only\n",
    "if all the pixels under the kernel is 1, otherwise it is eroded (made to zero).\n",
    "Hence, it causes the bondaries of the foreground object to erode away.\n",
    "- In this case, MORPH_OPEN is just another name of erosion followed by dilation,\n",
    "which is useful in removing noise.\n",
    "- This step applies morphology open with a vertical kernel to remove the thin lines from the dotted lines. A rectangular kernel is used(which has 3 width and 17 height)\n",
    "![morph2](morph2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 17)) #3,17\n",
    "morph = cv2.morphologyEx(morph, cv2.MORPH_OPEN, kernel)\n",
    "cv2.imshow(\"morph2\", cv2.resize(morph,(700,850)))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contours generation\n",
    "- Contours can be explained simply as a curve joining all the continuous points (along the boundary), \n",
    "having same color or intensity. The contours are a useful tool for shape analysis and object \n",
    "detection and recognition.\n",
    "- In OpenCV, finding contours is like finding white object from black background. So remember, \n",
    "object to be found should be white and background should be black. Here, the retrieval\n",
    "mode(second argument) used is the `cv2.RETR_EXTERNAL`, which retrieves only\n",
    "the extreme outer contours.\n",
    "- Contours are the boundaries of a shape with same intensity. It stores the (x,y) coordinates \n",
    "of the boundary of a shape. But does it store all the coordinates ? That is specified by this \n",
    "contour approximation method(third argument). Here we use `cv2.CHAIN_APPROX_SIMPLE`, \n",
    "which the contour is stored using only the four corners of the rectangle. This \n",
    "saves memory as compared to `cv2.CHAIN_APPROX_NONE`, which stores the contour \n",
    "using the entire outline of the rectangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[157, 673]],\n",
       " \n",
       "        [[156, 674]],\n",
       " \n",
       "        [[144, 674]],\n",
       " \n",
       "        [[144, 690]],\n",
       " \n",
       "        [[146, 690]],\n",
       " \n",
       "        [[147, 691]],\n",
       " \n",
       "        [[341, 691]],\n",
       " \n",
       "        [[341, 673]]], dtype=int32), array([[[  0, 571]],\n",
       " \n",
       "        [[  0, 613]],\n",
       " \n",
       "        [[187, 613]],\n",
       " \n",
       "        [[188, 612]],\n",
       " \n",
       "        [[212, 612]],\n",
       " \n",
       "        [[212, 593]],\n",
       " \n",
       "        [[211, 593]],\n",
       " \n",
       "        [[210, 592]],\n",
       " \n",
       "        [[210, 582]],\n",
       " \n",
       "        [[209, 581]],\n",
       " \n",
       "        [[188, 581]],\n",
       " \n",
       "        [[187, 580]],\n",
       " \n",
       "        [[187, 571]]], dtype=int32), array([[[  6, 538]],\n",
       " \n",
       "        [[  6, 558]],\n",
       " \n",
       "        [[225, 558]],\n",
       " \n",
       "        [[225, 539]],\n",
       " \n",
       "        [[178, 539]],\n",
       " \n",
       "        [[177, 538]]], dtype=int32), array([[[  0, 464]],\n",
       " \n",
       "        [[  0, 505]],\n",
       " \n",
       "        [[188, 505]],\n",
       " \n",
       "        [[188, 486]],\n",
       " \n",
       "        [[187, 485]],\n",
       " \n",
       "        [[187, 464]]], dtype=int32), array([[[ 17, 431]],\n",
       " \n",
       "        [[ 17, 451]],\n",
       " \n",
       "        [[216, 451]],\n",
       " \n",
       "        [[216, 432]],\n",
       " \n",
       "        [[186, 432]],\n",
       " \n",
       "        [[185, 431]]], dtype=int32), array([[[  0, 357]],\n",
       " \n",
       "        [[  0, 387]],\n",
       " \n",
       "        [[185, 387]],\n",
       " \n",
       "        [[185, 377]],\n",
       " \n",
       "        [[186, 376]],\n",
       " \n",
       "        [[188, 376]],\n",
       " \n",
       "        [[188, 357]]], dtype=int32), array([[[  0, 260]],\n",
       " \n",
       "        [[  0, 301]],\n",
       " \n",
       "        [[189, 301]],\n",
       " \n",
       "        [[189, 260]]], dtype=int32), array([[[  0, 163]],\n",
       " \n",
       "        [[  0, 205]],\n",
       " \n",
       "        [[189, 205]],\n",
       " \n",
       "        [[189, 184]],\n",
       " \n",
       "        [[190, 183]],\n",
       " \n",
       "        [[231, 183]],\n",
       " \n",
       "        [[231, 164]],\n",
       " \n",
       "        [[190, 164]],\n",
       " \n",
       "        [[189, 163]]], dtype=int32), array([[[ 87, 142]],\n",
       " \n",
       "        [[ 87, 160]],\n",
       " \n",
       "        [[ 88, 161]],\n",
       " \n",
       "        [[287, 161]],\n",
       " \n",
       "        [[288, 160]],\n",
       " \n",
       "        [[288, 143]],\n",
       " \n",
       "        [[207, 143]],\n",
       " \n",
       "        [[206, 142]]], dtype=int32), array([[[  0,  87]],\n",
       " \n",
       "        [[  0, 118]],\n",
       " \n",
       "        [[177, 118]],\n",
       " \n",
       "        [[177, 112]],\n",
       " \n",
       "        [[178, 111]],\n",
       " \n",
       "        [[263, 111]],\n",
       " \n",
       "        [[264, 112]],\n",
       " \n",
       "        [[276, 112]],\n",
       " \n",
       "        [[277, 113]],\n",
       " \n",
       "        [[277, 122]],\n",
       " \n",
       "        [[278, 123]],\n",
       " \n",
       "        [[495, 123]],\n",
       " \n",
       "        [[495, 104]],\n",
       " \n",
       "        [[494, 104]],\n",
       " \n",
       "        [[493, 103]],\n",
       " \n",
       "        [[493,  92]],\n",
       " \n",
       "        [[479,  92]],\n",
       " \n",
       "        [[478,  91]],\n",
       " \n",
       "        [[435,  91]],\n",
       " \n",
       "        [[434,  90]],\n",
       " \n",
       "        [[336,  90]],\n",
       " \n",
       "        [[335,  89]],\n",
       " \n",
       "        [[241,  89]],\n",
       " \n",
       "        [[240,  88]],\n",
       " \n",
       "        [[158,  88]],\n",
       " \n",
       "        [[157,  87]]], dtype=int32)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cntrs = cv2.findContours(morph, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "cntrs = cntrs[0] if len(cntrs) == 2 else cntrs[1]\n",
    "cntrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pytesseract on the contours\n",
    "- Loop through the contour list, and for each contour, extract the text from the contour image using Pytesseract\n",
    "- `cv2.rectangle` draws the contours as rectangle for display (0, 0, 255) is BGR for RED and 2 is the thickness.\n",
    "- Python-tesseract is an optical character recognition (OCR) tool for python. That is, it will recognize and “read” the text embedded in images.\n",
    "- `ROI` refers to the Region of Interest, which is specified by the contours found above.\n",
    "- As the contours found is not sorted, we append the text found into `ordered_values_tuples`(ordered list of tuples), with the text and the y_coordinate.\n",
    "- After that, we sort the `ordered_value_tuples` based on the y_coordinates Hence, it will now contain the text found in the image from top to bottom in order\n",
    "![result](result.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('For each question Nom 1 to 10, four options ara given, One of them ip the correct\\nanswer, Make oor hole (1,2 Sard) and anata your anewer oh tie Optical Arawor\\nSheet ‘(10 marks},', 87)\n",
      "('pa known you were i, A would not,', 142)\n",
      "('CO) Gaturb\\n@ astrpe\\n\\n( haddlorues\\n\\n(4) have dicuirbed', 163)\n",
      "('{i could she\\n2) weuttshe\\n@) count she\\nTh wouantene.', 260)\n",
      "('Ova\\n@) wats\\n(3) waited', 357)\n",
      "('4 TH bwing, sccompanied by inert\\necko.', 431)\n",
      "('oo &\\n@ ae\\nGas\\nG)owee', 464)\n",
      "('a ain later, the match\\nCON RAN', 538)\n",
      "('(WT\\nsha\\n@) vous\\nShow', 571)\n",
      "('WU NERSDETSITER Cor\\noe', 673)\n"
     ]
    }
   ],
   "source": [
    "ordered_value_tuples = []\n",
    "for c in cntrs:\n",
    "    area = cv2.contourArea(c)\n",
    "    x,y,w,h = cv2.boundingRect(c)\n",
    "    cv2.rectangle(img, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "\n",
    "    pytesseract.pytesseract.tesseract_cmd = \"C:/Program Files/Tesseract-OCR/tesseract.exe\"\n",
    "\n",
    "    image = cv2.imread(image_path, 0)\n",
    "    thresh = 255 - cv2.threshold(image, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "\n",
    "    ROI = thresh[y:y+h,x:x+w]\n",
    "\n",
    "    data = pytesseract.image_to_string(ROI, lang='eng',config='--psm 6')\n",
    "    \n",
    "    ordered_value_tuples.append((data, y))\n",
    "\n",
    "ordered_value_tuples.sort(key = lambda tup: tup[1])\n",
    "\n",
    "for value in ordered_value_tuples:\n",
    "    print(value)\n",
    "    \n",
    "cv2.imshow(\"result\", cv2.resize(img,(700,850)))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
